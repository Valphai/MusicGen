{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPW9bWa0Vxe22gX1vgkItPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valphai/MusicGen/blob/main/MusicGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNr1406AHRnO"
      },
      "source": [
        "# DEPENDENCIES\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from IPython import display as Idisplay\n",
        "import os\n",
        "!wget -P /christmas http://www.stephenmerrony.co.uk/ABC/Carols/ABC_Carols_v1.4.zip\n",
        "!unzip /christmas/ABC_Carols_v1.4 -d /christmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJAdBkIwhMRC"
      },
      "source": [
        "def load_data():\n",
        "  rootdir = \"/christmas/\"\n",
        "  songs_list = []\n",
        "  for subdir, dirs, files in os.walk(rootdir):\n",
        "    for abc in files:\n",
        "      with open(os.path.join(subdir, abc)) as f:\n",
        "        song = list(f.readlines())[4:]\n",
        "        for i, line in enumerate(song):\n",
        "          if line.startswith((\"V:\", \"w:\", \"%\", \"Z:\")):\n",
        "            del song[i]\n",
        "        if len(song) < 30:\n",
        "          songs_list.append(\"\".join(song))\n",
        "  return songs_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sevr-S3Shyd0"
      },
      "source": [
        "# checking the validity of the data\n",
        "try:\n",
        "  songs_list = load_data()\n",
        "except:\n",
        "  os.remove(\"/christmas/ABC_Carols_v1.4.zip\")\n",
        "  songs_list = load_data()\n",
        "\n",
        "print(songs_list[0])\n",
        "print(songs_list[0].__len__())\n",
        "\n",
        "# joining all the songs into one string and extracting elements\n",
        "songs = \"\\n\\n\".join(songs_list)\n",
        "vocabulary = sorted(set(songs))\n",
        "print(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kncj6z2ynsoj"
      },
      "source": [
        "# dictionary of what element is what number\n",
        "char2idx = {char : num for num, char in enumerate(vocabulary)}\n",
        "idx2char = np.array(vocabulary)\n",
        "\n",
        "# vectorize the text (make chars into ints)\n",
        "def vectorize(string):\n",
        "  return np.array([char2idx[i] for i in string])\n",
        "\n",
        "vectorized_songs = vectorize(songs)\n",
        "print(vectorized_songs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS4PWhMdXMtJ"
      },
      "source": [
        "VOCAB_SIZE = len(vocabulary)\n",
        "SEQ_LENGTH = 100\n",
        "EPOCHS = 2000\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "checkpoint_path = \"/christmas_checkpoint\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_path, \"chkpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gmB289BWxEj"
      },
      "source": [
        "def my_model(vocab_size, batch_size=32, output_dim=256, lstm_units=1024):\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.Embedding(input_dim=vocab_size, output_dim=output_dim, \n",
        "                             batch_input_shape=[batch_size, None]),\n",
        "      keras.layers.LSTM(lstm_units, return_sequences=True, stateful=True),\n",
        "      keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWOO2_PhXHVp"
      },
      "source": [
        "model = my_model(VOCAB_SIZE)\n",
        "model.summary() # the model takes in tensor of shape (BATCH_SIZE, SEQ_LENGTH, VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjkiYe4gSyhK"
      },
      "source": [
        "class Custom_training():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  def gradient_desc(self, inputs, labels):\n",
        "    \"\"\"\n",
        "    Calculate gradients and compute losses during network training.\n",
        "    inputs and labels are given by get_batches method. \n",
        "    This method is used to calculate and return the loss during\n",
        "    an epoch.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      # y_hat\n",
        "      predictions = model(inputs)\n",
        "\n",
        "      loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, \n",
        "                                                      from_logits=True)\n",
        "      \n",
        "      # derivatives\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      Custom_training.optimizer.apply_gradients(\n",
        "                                        zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  def training(self, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      inputs, labels = self.get_batches(SEQ_LENGTH)\n",
        "      loss = self.gradient_desc(inputs, labels)\n",
        "\n",
        "      if epoch % 500 == 0:\n",
        "        model.save_weights(checkpoint_prefix)\n",
        "    model.save_weights(checkpoint_prefix)\n",
        "\n",
        "  def get_batches(self, seq_len, batch_size=32):\n",
        "    \"\"\"\n",
        "    This method splits data to get inputs and labels. The idea here\n",
        "    is to pick random index in the dataset and grab a few next letters\n",
        "    so that the model can learn. This is done once per epoch.\n",
        "    \"\"\"\n",
        "    n = vectorized_songs.shape[0] - 1\n",
        "    indx = np.random.choice(n - seq_len, batch_size)\n",
        "\n",
        "    inputs = [vectorized_songs[i : i + seq_len] for i in indx]\n",
        "    labels = [vectorized_songs[i + 1 : i + 1 + seq_len] for i in indx]\n",
        "\n",
        "    input_batch = np.reshape(inputs, [batch_size, seq_len])\n",
        "    label_batch = np.reshape(labels, [batch_size, seq_len])\n",
        "    return input_batch, label_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK92UBI7PBhD"
      },
      "source": [
        "# training the model\n",
        "Custom_training().training(epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Pu5VXyLZrH"
      },
      "source": [
        "# rebuilding the model & loading the weights\n",
        "model = my_model(VOCAB_SIZE, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_path))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh_grzmRNk7"
      },
      "source": [
        "# get back chars from ints\n",
        "def generate_song(generation_length=1000):\n",
        "  \"\"\"\n",
        "  The model makes word predictions.\n",
        "  \"\"\"\n",
        "  model.reset_states()\n",
        "  generated_song = []\n",
        "  # start input\n",
        "  start = \"X\"\n",
        "  input = tf.expand_dims([char2idx[start]], 0) # (1, 1, 1)\n",
        "  for i in range(generation_length):\n",
        "    prediction = model(input)\n",
        "\n",
        "    prediction = tf.squeeze(prediction, 0)\n",
        "\n",
        "    # sample the output logits to generate token IDs.\n",
        "    predicted_id = tf.random.categorical(prediction, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # change generator input passed to the model within the loop\n",
        "    input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    generated_song.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start + \"\".join(generated_song))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfRK_QZLYGtP"
      },
      "source": [
        "generated_song = generate_song(1000)\n",
        "print(generated_song)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8GfsE-y2RbF"
      },
      "source": [
        "## Refference\n",
        "\n",
        "1. TensorFlow Core \"Text Generation with an RNN\", www.tensorflow.org/tutorials/text/text_generation."
      ]
    }
  ]
}